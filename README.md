# Non-acted-Multi-view-Audio-Visual-Dyadic-Interactions-Project
This repository contains all code generated during the development of the master thesis Non-acted Multi-view Audio-Visual Dyadic Interactions Project during the spring semester of the Master in Foundations of Data Science 2018/2019. By Aleix Casellas, Andreu Masdeu, Pablo Lázaro and Rubén Barco.


## Pablo Lázaro Herrasti
### Master thesis name

Contributions

## Rubén Barco Terrones
### Multi-modal Local and Recurrent Non-verbal Emotion Recognition in Dyadic Scenarios

Study of the state-of-the-art of the emotion recognition problem using audiovisual sources and the different techniques that make use or not about the context, temporal information, memory blocks, etc. Deliver a emotion recognition system using Deep Learning techniques based on unimodal handcraft audio features, raw audio features and faces, and their possible fusion. Also study the influence of the temporal information to model the changes across frames in the emotion recognition problem in the unimodal and fusion experiments using RNNs. Help and participate during the recordings of the different sessions of the \textit{Face-to-face Dyadic Interaction Dataset} placing and collecting the setup and attending the participants. Also annotate this database labeling the utterances of the videos.

## Aleix Casellas Comas
### Master thesis name

Contributions

## Andreu Masdue Ninot
### Multitask Learning for Facial Attributes Analysis

Contributions
