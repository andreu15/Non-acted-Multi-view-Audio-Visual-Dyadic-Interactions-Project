# Non-acted Multi-view Audio-Visual Dyadic Interactions Project
This repository contains all code generated during the development of the master thesis Non-acted Multi-view Audio-Visual Dyadic Interactions Project during the spring semester of the Master in Foundations of Data Science 2018/2019. By Aleix Casellas, Andreu Masdeu, Pablo Lázaro and Rubén Barco.


## Pablo Lázaro Herrasti
### Multi-modal Local Non-verbal Emotion Recognition in Dyadic scenarios and Speaker Segmentation

Study of the state-of-the-art of the emotion recognition problem using audiovisual sources. Deliver a emotion recognition system using Deep Learning techniques based on unimodal audio features, raw audio and faces, and their possible fusion. On the other hand, study of state-of-the-art of the speaker segmentation problem using audio sources and unsupervised learning techniques such as **Spectral Clustering**. Help and participate during the recordings of the different sessions of the **Face-to-face Dyadic Interaction Dataset}** placing and collecting the setup and attending the participants. Also annotate this database labeling the utterances of the videos.

## Rubén Barco Terrones
### Multi-modal Local and Recurrent Non-verbal Emotion Recognition in Dyadic Scenarios

Study of the state-of-the-art of the **emotion recognition** problem using audiovisual sources and the different techniques that make use or not about the context, temporal information, memory blocks, etc. Deliver a emotion recognition system using Deep Learning techniques based on unimodal handcraft audio features, raw audio features and faces, and their possible fusion. Also study the influence of the temporal information to model the changes across frames in the emotion recognition problem in the unimodal and fusion experiments using **RNNs**. Help and participate during the recordings of the different sessions of the **Face-to-face Dyadic Interaction Dataset** placing and collecting the setup and attending the participants. Also annotate this database labeling the utterances of the videos.

## Aleix Casellas Comas
### Master thesis name

Contributions

## Andreu Masdeu Ninot
### Multitask Learning for Facial Attributes Analysis

In this thesis we explore the use of Multitask Learning for improving performance in facial attributes tasks such as gender, age and ethnicity prediction. These tasks, along with emotion recognition will be part of a new dyadic interaction dataset which was recorded during the development of this thesis. This work includes the implementation of two state of the art multitask deep learning models and the discussion of the results obtained from these methods in a preliminary dataset, as well as a first evaluation in a sample of the dyadic interaction dataset. This will serve as a baseline for a future implementation of Multitask Learning methods in the fully annotated dyadic interaction dataset.

The contribution of this master thesis within the whole project is the study of the state of the art in multitask deep learning, specially for face attributes analysis. Training and analysis of two state of the art multitask learning architectures on an external dataset and a cross-database evaluation of these trained models in the Dyadic Interaction Dataset. Comparision between multitask learning models and with single-task learning models. Help and participation during the recordings of the different sessions of the Face-to-face Dyadic Interaction Dataset placing and collecting the setup and attending the participants. Also collaboration in the annotation of this database labelling the
emotions of the participants.

In the MultitaskLearning folder you can find the implementation of all models in the buildings subfolder, scripts for training the models in the train subfolder and notebooks to analyze the results in the analysis subfolder.
